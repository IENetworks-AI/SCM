version: "3.9"

services:
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    environment:
      KAFKA_CLUSTERS_0_NAME: "local"
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "scm_pipeline-kafka-1:9092"
    ports:
      - "8085:8080"
    restart: unless-stopped  
  scm_ml-api:
    build:
      context: ./ml-api
      dockerfile: Dockerfile
    container_name: scm_ml-api
    networks:
      - scm_pipeline_default
    ports:
      - "8001:8001"
    environment:
      - SOME_ENV_VAR=value
    volumes:
      - ./scripts:/app/scripts 

  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 5s
      timeout: 5s
      retries: 5

  zookeeper:
    image: bitnami/zookeeper:3.9
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD", "echo", "ok"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - zookeeper_data:/bitnami

  kafka:
    image: bitnami/kafka:3.7
    depends_on:
      zookeeper:
        condition: service_started
    environment:
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,PLAINTEXT_HOST://:29092
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
      ALLOW_PLAINTEXT_LISTENER: "yes"
    ports:
      - "9092:9092"
      - "29092:29092"
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - kafka_data:/bitnami

  airflow-init:
    build: ./airflow
    depends_on:
      postgres:
        condition: service_healthy
    user: "${AIRFLOW_UID:-1000}:0"
    environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    - AIRFLOW_UID=${AIRFLOW_UID:-1000}
    - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/data:/opt/airflow/data
      - ./airflow/requirements.txt:/requirements.txt
      - ./.env:/opt/airflow/.env
      - ./airflow/output:/opt/airflow/output
      - ./airflow/logs:/opt/airflow/logs
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true
      "

  airflow-webserver:
    build: ./airflow
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      kafka:
        condition: service_started
    restart: always
    user: "${AIRFLOW_UID:-1000}:0"
    environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    - AIRFLOW_UID=${AIRFLOW_UID:-1000}
    - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    ports:
      - "8082:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/data:/opt/airflow/data
      - ./airflow/requirements.txt:/requirements.txt
      - ./.env:/opt/airflow/.env
      - ./airflow/output:/opt/airflow/output
      - ./airflow/logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts
    command: airflow webserver

  airflow-scheduler:
    build: ./airflow
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      kafka:
        condition: service_started
    restart: always
    user: "${AIRFLOW_UID:-1000}:0"
    environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    - AIRFLOW_UID=${AIRFLOW_UID:-1000}
    - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/data:/opt/airflow/data
      - ./airflow/requirements.txt:/requirements.txt
      - ./.env:/opt/airflow/.env
      - ./airflow/output:/opt/airflow/output
      - ./airflow/logs:/opt/airflow/logs
    command: airflow scheduler

  streamlit:
    build:
      context: ./streamlit
      dockerfile: Dockerfile
    container_name: scm_pipeline-streamlit-1
    depends_on:
      kafka:
        condition: service_started
    networks:
      - scm_pipeline_default
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - GEMINI_API_KEY=AIzaSyBTB4ooWmFCwf62wxJGcwWhP-QZzrqKNsU
    volumes:
      - ./streamlit:/app
      - ./streamlit/requirements.txt:/app/requirements.txt
    ports:
      - "8501:8501"
    command: bash -c "pip install -r /app/requirements.txt && streamlit run /app/streamlit_app.py --server.port 8501"

volumes:
  postgres_data:
  zookeeper_data:
  kafka_data:
  airflow_output:
  airflow_logs:

networks:
  scm_pipeline_default:
    external: true

